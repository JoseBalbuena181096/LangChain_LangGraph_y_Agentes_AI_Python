Para ejecutarlo debemos

Activar el ambiente adecuado:
```bash
conda activate llms
```

Ingresar a la ruta adecuada

```bash
cd '/home/jose/Ingenieria_de_LLM/LangChain_LangGraph_y_Agentes_AI_Python' 
```
Exportar las variables de ambiente:

```bash
export $(cat .env | xargs)
```

Ejecutar el script:
```bash
python 1_Intro_LangChain/hello_word.py
``` 

Podemos tambien usar python-dotenv (mÃ¡s profesional):
```bash
pip install python-dotenv
```

Y luego modificar el script:
```python
from dotenv import load_dotenv

load_dotenv()

import os

print(os.getenv("OPENAI_API_KEY"))
```

Como el de google ya lo hemos configurado para que carge las variables de ambiente, no es necesario exportarlas manualmente. Se ejecuta asi de rapido:
```bash
python /home/jose/Ingenieria_de_LLM/LangChain_LangGraph_y_Agentes_AI_Python/1_Intro_LangChain/hello_word_Gemini.py

```

Las plantillas en langchain son una forma de crear prompts personalizados.
- Posemos usar variables en las plantillas para personalizar el prompt.
- Posemos usar condicionales en las plantillas para controlar el flujo del prompt.
- Posemos usar bucles en las plantillas para repetir partes del prompt.
- Las variables en las plantillas pueden ser usadas para pasar informaciÃ³n dinÃ¡mica al prompt.


Las cadenas son una secuencia de pasos que nos permiten crear prompts personalizados, por medio de cadenas secuenciales.

LCE: LangChain Expression Language
Son una forma de definir cadenas secuenciales de manera mÃ¡s concisa y legible.


## Lectura: Arquitectura de Paquetes en LangChain
En esta lecciÃ³n nos detendremos a comprender la arquitectura general de LangChain y su ecosistema al dÃ­a de hoy.

Esto nos darÃ¡ un mapa mental de cÃ³mo encajan las piezas: modelos, cadenas, agentes, memoria, herramientas, y tambiÃ©n cÃ³mo LangGraph expande las capacidades de LangChain. Aunque no escribamos mucho cÃ³digo nuevo aquÃ­, este conocimiento conceptual serÃ¡ muy Ãºtil cuando abordemos proyectos mÃ¡s complejos en secciones posteriores.

Arquitectura de Paquetes LangChain

Antes de entrar en los componentes especÃ­ficos, es importante entender cÃ³mo se organiza LangChain. El ecosistema se ha modularizado significativamente para mejorar la mantenibilidad, reducir dependencias y permitir instalaciones mÃ¡s ligeras:

Estructura modular:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AplicaciÃ³n de Usuario                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      langchain                          â”‚
â”‚  (Chains, Agents, Memory, Callbacks de alto nivel)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚ â”‚               â”‚ â”‚                â”‚
â”‚langchain-coreâ”‚ â”‚  langchain-   â”‚ â”‚Partner Packagesâ”‚
â”‚              â”‚ â”‚  community    â”‚ â”‚                â”‚
â”‚ Abstraccionesâ”‚ â”‚ Integraciones â”‚ â”‚ - openai       â”‚
â”‚ Interfaces   â”‚ â”‚ de terceros   â”‚ â”‚ - anthropic    â”‚
â”‚ LCEL         â”‚ â”‚               â”‚ â”‚ - ollama       â”‚
â”‚              â”‚ â”‚               â”‚ â”‚ - groq         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Los cuatro niveles principales:

â€¢ langchain-core: Contiene las abstracciones fundamentales e interfaces base. AquÃ­ estÃ¡n las clases abstractas como BaseLanguageModel, BasePromptTemplate, Runnable, y el corazÃ³n del LCEL (LangChain Expression Language). Es la base sobre la que se construye todo lo demÃ¡s.

â€¢ langchain-community: Integraciones con servicios y herramientas de terceros que no tienen soporte oficial optimizado. Incluye conectores para bases de datos vectoriales de cÃ³digo abierto, loaders de documentos, herramientas diversas, etc.

â€¢ Partner Packages: Paquetes separados mantenidos oficialmente para integraciones especÃ­ficas con proveedores principales (langchain-openai, langchain-anthropic, langchain-ollama, etc.). Estos estÃ¡n optimizados y tienen mejor soporte que las integraciones genÃ©ricas.

Ventajas de esta arquitectura:

Instalaciones ligeras: Solo instalas lo que necesitas. Si solo usas OpenAI, instalas langchain-core y langchain-openai.

Mejor mantenimiento: Cada integraciÃ³n puede evolucionar independientemente.

Menos conflictos de dependencias: Las integraciones especÃ­ficas no afectan al core.

Actualizaciones mÃ¡s rÃ¡pidas: Los partner packages se pueden actualizar sin esperar releases del paquete principal.

Implicaciones prÃ¡cticas:

En la actualidad, cuando veas ejemplos de cÃ³digo, notarÃ¡s imports como:
```python

from langchain_core.prompts import ChatPromptTemplate  # Abstracciones base
from langchain.chains import ConversationChain         # Funcionalidad de alto nivel
from langchain_openai import ChatOpenAI                # IntegraciÃ³n especÃ­fica
from langchain_community.vectorstores import FAISS    # Herramienta de terceros
```


Esta estructura te ayuda a entender quÃ© estÃ¡ pasando bajo el capÃ³ y te permite hacer instalaciones mÃ¡s selectivas segÃºn tus necesidades.


##  Lectura: Â¿QuÃ© es Streamlit?
Antes de sumergirnos en la construcciÃ³n del chatbot, necesitamos entender quÃ© hace especial a Streamlit y por quÃ© es la herramienta perfecta para nuestro proyecto. Streamlit es, en esencia, la respuesta a una pregunta que todo cientÃ­fico de datos o desarrollador de IA se ha hecho alguna vez: "Â¿Por quÃ© necesito aprender JavaScript, HTML y CSS solo para mostrar mi modelo en funcionamiento?"

La filosofÃ­a del "script que se convierte en app"
Streamlit opera bajo una premisa revolucionaria: tu script de Python es tu aplicaciÃ³n web. No hay separaciÃ³n entre lÃ³gica de backend y frontend, no hay archivos de configuraciÃ³n complejos ni servidores que montar. Escribes cÃ³digo Python normal y corriente, aÃ±ades unas pocas funciones que empiezan con st., y mÃ¡gicamente tienes una aplicaciÃ³n web interactiva.

Esta simplicidad no es casualidad. Los creadores de Streamlit observaron que la mayorÃ­a de aplicaciones de datos siguen un patrÃ³n similar: muestran informaciÃ³n, permiten al usuario interactuar con controles, y actualizan la vista. En lugar de obligarte a manejar estados complejos y ciclos de vida de componentes, Streamlit dice: "Dime quÃ© quieres mostrar, yo me encargo del resto."

El ciclo reactivo: la magia detrÃ¡s del telÃ³n
El corazÃ³n de Streamlit es su modelo de ejecuciÃ³n reactiva. Cada vez que el usuario interactÃºa con tu aplicaciÃ³n (presiona un botÃ³n, escribe en un campo, mueve un slider) Streamlit vuelve a ejecutar tu script completo de arriba a abajo. Este "rerun" recalcula toda la interfaz basÃ¡ndose en el nuevo estado.

Para quienes vienen del desarrollo web tradicional, esto puede sonar ineficiente. Â¿Reejecutar todo el cÃ³digo en cada interacciÃ³n? Pero Streamlit ha optimizado este proceso de tal manera que se siente instantÃ¡neo, y la simplicidad conceptual que aporta es inmensa: no tienes que pensar en quÃ© partes de tu interfaz necesitan actualizarse; Streamlit se encarga de todo.

Estado persistente: la memoria de tu aplicaciÃ³n
AquÃ­ surge una pregunta natural: si el script se ejecuta desde cero en cada interacciÃ³n, Â¿cÃ³mo mantenemos informaciÃ³n entre clicks? La respuesta es st.session_state, un diccionario especial que Streamlit mantiene en memoria para cada sesiÃ³n de usuario.

Piensa en st.session_state como la memoria a corto plazo de tu aplicaciÃ³n. AhÃ­ puedes guardar el historial de mensajes de tu chatbot, contadores, configuraciones del usuario, o cualquier dato que necesite sobrevivir al siguiente rerun. Es tu espacio personal de almacenamiento por sesiÃ³n de navegador.

Componentes listos para usar: enfÃ³cate en lo importante
Una de las grandes fortalezas de Streamlit es su biblioteca de componentes pre-construidos. Para nuestro chatbot, esto significa que no necesitaremos diseÃ±ar burbujas de chat, manejar layouts complejos, o crear campos de entrada personalizados. Streamlit incluye st.chat_message() para mostrar mensajes con el formato visual correcto, y st.chat_input() para capturar la entrada del usuario con el estilo de un chat moderno.

Esta filosofÃ­a de "componentes inteligentes" se extiende por toda la librerÃ­a: grÃ¡ficos con st.plotly_chart(), tablas con st.dataframe(), controles de archivo con st.file_uploader(). Cada funciÃ³n encapsula no solo la funcionalidad, sino tambiÃ©n las mejores prÃ¡cticas de diseÃ±o UX.

Declarativo y progresivo: cÃ³digo que se lee como se ve
El diseÃ±o en Streamlit es declarativo: el orden en que escribes las funciones st.* es exactamente el orden en que aparecen en la pantalla. Â¿Quieres un tÃ­tulo, despuÃ©s una descripciÃ³n, luego el chat, y finalmente el input del usuario? Simplemente escrÃ­belos en ese orden. No hay divs, no hay CSS, no hay posicionamiento absoluto. Tu cÃ³digo se lee como un mapa visual de tu aplicaciÃ³n.

AdemÃ¡s, Streamlit es progresivo. Puedes empezar con la estructura mÃ¡s simple (una columna de elementos apilados) y despuÃ©s aÃ±adir complejidad: columnas lado a lado con st.columns(), pestaÃ±as con st.tabs(), o barras laterales con st.sidebar. Pero para nuestro chatbot, la simplicidad lineal serÃ¡ perfecta.

El ciclo de vida de una conversaciÃ³n
En una aplicaciÃ³n de chat con Streamlit, el flujo mental es siempre predecible:

InicializaciÃ³n: Verificas si st.session_state tiene el historial de mensajes; si no, lo creas vacÃ­o.

Renderizado del historial: Recorres los mensajes guardados y los muestras con st.chat_message().

Captura de entrada: Usas st.chat_input() para obtener el nuevo mensaje del usuario.

Procesamiento: Si hay un mensaje nuevo, lo aÃ±ades al historial, consultas tu modelo de IA, y aÃ±ades tambiÃ©n la respuesta.

ActualizaciÃ³n automÃ¡tica: Streamlit detecta los cambios en session_state y vuelve a ejecutar el script, mostrando la conversaciÃ³n actualizada.

Este ciclo, repetido en cada turno, crea la ilusiÃ³n de una conversaciÃ³n fluida, cuando en realidad son mÃºltiples ejecuciones independientes del mismo script.

Secretos y configuraciÃ³n: seguridad sin complejidad
Las aplicaciones reales necesitan manejar credenciales: API keys, tokens de acceso, configuraciones sensibles. Streamlit resuelve esto con su sistema de "secretos", que puede leer tanto de variables de entorno del sistema como de archivos de configuraciÃ³n especiales cuando despliegas en Streamlit Cloud.

La regla de oro es simple: las claves nunca van en el cÃ³digo. Streamlit facilita este principio con st.secrets, que actÃºa como un diccionario donde puedes acceder a tus credenciales de forma segura, sin exponerlas en tu repositorio.

CachÃ©s inteligentes: optimizaciÃ³n cuando la necesites
Aunque nuestro proyecto inicial no lo requerirÃ¡, Streamlit incluye decoradores de cachÃ© sofisticados: @st.cache_data para datos y @st.cache_resource para objetos complejos como modelos de ML. Estos mecanismos evitan recÃ¡lculos costosos en cada rerun, manteniendo la responsividad de tu aplicaciÃ³n incluso cuando grows.

El cachÃ© de Streamlit es inteligente: detecta automÃ¡ticamente cuando los parÃ¡metros de entrada cambian y solo entonces vuelve a ejecutar la funciÃ³n. Para cargar un modelo pesado o procesar un dataset grande, estos decoradores son invaluables.

Streamlit vs. el mundo web tradicional
Streamlit no pretende competir con React + Flask para aplicaciones web de producciÃ³n a gran escala. Su propuesta de valor es diferente: ser el puente mÃ¡s rÃ¡pido entre una idea y una demostraciÃ³n interactiva. Es el equivalente web de un Jupyter notebook: perfecto para explorar, prototipar y compartir, con la ventaja adicional de una interfaz amigable para usuarios no tÃ©cnicos.

Para nuestro chatbot, esta filosofÃ­a es perfecta. No necesitamos escalabilidad extrema ni arquitecturas complejas; necesitamos iterar rÃ¡pido y mantener el foco en la lÃ³gica de conversaciÃ³n, no en los detalles de presentaciÃ³n.

Lo que Streamlit nos regala en este proyecto
Resumiendo, Streamlit nos aporta exactamente lo que necesitamos:

Una interfaz de chat lista para usar, sin maquetaciÃ³n manual ni CSS.

Estado persistente por sesiÃ³n que mantiene viva la conversaciÃ³n entre interacciones.

Un modelo mental simple: cada rerun es una nueva oportunidad de mostrar el estado actual.

Extensibilidad natural: podemos empezar simple y aÃ±adir complejidad gradualmente.

Un camino claro hacia el despliegue: de prototipo local a aplicaciÃ³n compartible en la nube.

Con este entendimiento, en la siguiente secciÃ³n podremos concentrarnos completamente en la lÃ³gica del chatbot y su integraciÃ³n con LangChain, confiando en que Streamlit traducirÃ¡ nuestras intenciones en una interfaz clara y funcional para nuestros usuarios.

## Para ejecutar un streamlit app
Para ejecutar la app, abre una terminal y navega hasta el directorio donde se encuentra el archivo `streamlit_chatbot.py`. Luego, ejecuta el siguiente comando:

```bash
streamlit run streamlit_chatbot.py
```

Esto iniciarÃ¡ la app en tu navegador predeterminado. Puedes interactuar con el chatbot escribiendo mensajes en la caja de texto y presionando Enter. La conversaciÃ³n se mostrarÃ¡ en la interfaz, y el historial se guardarÃ¡ en `st.session_state`.

## Langchain tipos de mensaje
AI: Mensaje generado por el modelo de IA.
Human: Mensaje enviado por el usuario.
System: Mensaje que define el comportamiento del modelo.

Tarea: Mejoras adicionales: LCEL, Prompt Templates, Streaming...
ğŸ“‹ Objetivo

En esta tarea transformarÃ¡s tu chatbot bÃ¡sico en una aplicaciÃ³n mÃ¡s robusta y profesional. AprenderÃ¡s a implementar funcionalidades como configuraciÃ³n dinÃ¡mica, templates de prompts, streaming de respuestas y mejor manejo de errores.

âš ï¸ Importante: Esta tarea contiene conceptos avanzados y es completamente normal que no puedas completar todos los puntos en tu primer intento. No te preocupes si encuentras dificultades, el objetivo es que explores, experimentes y aprendas el proceso. La soluciÃ³n completa estÃ¡ disponible en la siguiente clase del curso para que puedas consultar y comparar tu progreso. Â¡Haz lo que puedas y disfruta el proceso de aprendizaje!



ğŸ¯ Lo que aprenderÃ¡s

PromptTemplate: CÃ³mo crear prompts estructurados y reutilizables

LCEL (LangChain Expression Language): El nuevo paradigma de cadenas en LangChain

Streaming: Respuestas en tiempo real para mejor experiencia de usuario

ConfiguraciÃ³n dinÃ¡mica: Permitir al usuario ajustar parÃ¡metros del modelo

Manejo de errores: Hacer tu aplicaciÃ³n mÃ¡s robusta



ğŸ Punto de partida

Tu cÃ³digo actual deberÃ­a verse similar al siguiente:

from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
import streamlit as st
 
# Configurar la pÃ¡gina de la app
st.set_page_config(page_title="Chatbot BÃ¡sico", page_icon="ğŸ¤–")
st.title("ğŸ¤– Chatbot BÃ¡sico con LangChain")
st.markdown("Este es un *chatbot de ejemplo* construido con LangChain + Streamlit. Â¡Escribe tu mensaje abajo para comenzar!")
 
chat_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
 
# Inicializar el historial de mensajes
if "mensajes" not in st.session_state:
    st.session_state.mensajes = []
 
# Mostrar mensajes previos en la interfaz
for msg in st.session_state.mensajes:
    if isinstance(msg, SystemMessage):
        # No muestro el mensaje por pantalla
        continue
    
    role = "assistant" if isinstance(msg, AIMessage) else "user"
    with st.chat_message(role):
        st.markdown(msg.content)
 
# Cuadro de entrada de texto de usuario
pregunta = st.chat_input("Escribe tu mensaje: ")
 
if pregunta:
    # Mostrar inmediatamente el mensaje del usuario en la interfaz
    with st.chat_message("user"):
        st.markdown(pregunta)
    
    # Almacenamos el mensaje en la memoria de streamlit
    st.session_state.mensajes.append(HumanMessage(content=pregunta))
    
    # Generar respuesta usando el modelo de lenguaje
    respuesta = chat_model.invoke(st.session_state.mensajes)
    
    # Mostrar la respuesta en la interfaz
    with st.chat_message("assistant"):
        st.markdown(respuesta.content)
    
    st.session_state.mensajes.append(respuesta)


ğŸ› ï¸ Mejoras a implementar

1. Sidebar de ConfiguraciÃ³n

Objetivo: Permitir al usuario ajustar la temperatura y seleccionar el modelo.

Pista: Usa st.sidebar para crear una barra lateral. Dentro de ella:

with st.sidebar:
    st.header("ConfiguraciÃ³n")
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.5, 0.1)
    model_name = st.selectbox("Modelo", ["gpt-3.5-turbo", "gpt-4", "gpt-4o-mini"])
    
    # Â¿CÃ³mo recrearÃ­as el modelo con los nuevos parÃ¡metros?
    chat_model = # Â¡Completa aquÃ­!


Pregunta reflexiva: Â¿Por quÃ© es Ãºtil recrear el modelo cada vez que cambian los parÃ¡metros?



2. Implementando PromptTemplate

Objetivo: Crear un template estructurado que incluya el historial de conversaciÃ³n.

ImportaciÃ³n necesaria:

from langchain.prompts import PromptTemplate

Tu turno: Crea un PromptTemplate que:

Tenga variables mensaje e historial

Defina la personalidad del chatbot

Use el historial para mantener contexto

prompt_template = PromptTemplate(
    input_variables=["mensaje", "historial"],
    template="""Eres un asistente Ãºtil y amigable llamado ChatBot Pro. 
 
Historial de conversaciÃ³n:
{historial}
 
Responde de manera clara y concisa a la siguiente pregunta: {mensaje}"""
)


3. LCEL - LangChain Expression Language

Objetivo: Usar el nuevo paradigma de cadenas con el operador |.

Concepto clave: LCEL permite encadenar componentes de forma intuitiva:

# En lugar de usar cadenas tradicionales, ahora puedes hacer:
cadena = prompt_template | chat_model


Â¿QuÃ© hace esto?: El operador | conecta el output del PromptTemplate directamente al input del ChatModel.



4. Streaming de Respuestas

Objetivo: Mostrar la respuesta del modelo palabra por palabra, como ChatGPT.

ImplementaciÃ³n sugerida:

if pregunta:
    with st.chat_message("user"):
        st.markdown(pregunta)
    
    try:
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
 
            # Â¡AquÃ­ estÃ¡ la magia del streaming!
            for chunk in cadena.stream({"mensaje": pregunta, "historial": st.session_state.mensajes}):
                full_response += chunk.content
                response_placeholder.markdown(full_response + "â–Œ")  # El cursor parpadeante
            
            response_placeholder.markdown(full_response)
        
        # No olvides almacenar los mensajes
        st.session_state.mensajes.append(HumanMessage(content=pregunta))
        st.session_state.mensajes.append(AIMessage(content=full_response))
        
    except Exception as e:
        # Â¿QuÃ© tipo de errores podrÃ­an ocurrir aquÃ­?
        st.error(f"Error al generar respuesta: {str(e)}")
        st.info("Verifica que tu API Key de OpenAI estÃ© configurada correctamente.")


Pregunta: Â¿QuÃ© representa el sÃ­mbolo "â–Œ" en el cÃ³digo anterior?



5. BotÃ³n de Nueva ConversaciÃ³n

Objetivo: Permitir al usuario limpiar el historial fÃ¡cilmente.

Pista simple:

if st.button("ğŸ—‘ï¸ Nueva conversaciÃ³n"):
    # Â¿QuÃ© necesitas limpiar?
    # Â¿QuÃ© funciÃ³n de Streamlit refresca la pÃ¡gina?


ğŸ’¡ Reflexiones Finales

Â¿QuÃ© ventajas tiene usar PromptTemplate vs. strings simples?

Â¿CÃ³mo mejora LCEL la legibilidad del cÃ³digo?

Â¿Por quÃ© el streaming mejora la experiencia de usuario?

Â¿QuÃ© otros errores podrÃ­as manejar en una aplicaciÃ³n real?



Â¡DiviÃ©rtete construyendo y no dudes en experimentar con las funcionalidades! ğŸš€