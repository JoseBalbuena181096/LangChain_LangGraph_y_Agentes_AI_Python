## Componentes clave

Los principales componentes clave de lanchaing, son 
- Chain: Secuencia de llamadas a componentes, como modelos de lenguaje, herramientas externas y otras cadenas.
- Runnable: Objeto que puede ser invocado para ejecutar una secuencia de llamadas a componentes. Recibe una serie de entradas y produce una serie de salidas.

Langchaing nos permite unir diferentes runnables en una secuencia, para formar una cadena de llamadas.


## Tarea: Mini-Proyecto: An√°lisis de sentimientos con LangChain
üìã Objetivo

En esta tarea construir√°s desde cero un sistema completo de an√°lisis de sentimientos usando LangChain. Aprender√°s a crear funciones personalizadas, convertirlas en objetos Runnable usando RunnableLambda, y combinarlas en cadenas de procesamiento complejas.

‚ö†Ô∏è Importante: Esta tarea introduce conceptos avanzados de LangChain y es completamente normal que encuentres dificultades. No te preocupes si no puedes completar todos los puntos, el objetivo principal es que practiques el uso de RunnableLambda y entiendas c√≥mo encadenar operaciones. La soluci√≥n completa estar√° disponible en la siguiente clase para consulta. ¬°Experimenta, aprende del proceso y divi√©rtete construyendo!



üéØ Lo que aprender√°s

RunnableLambda: Convertir funciones Python en objetos Runnable

Cadenas complejas: Combinar m√∫ltiples operaciones de procesamiento

Manejo de JSON: Estructurar respuestas del modelo

Preprocesamiento: Limpiar y preparar texto para an√°lisis

Arquitectura modular: Dividir problemas complejos en pasos simples



üèóÔ∏è Arquitectura del Sistema

Tu sistema final tendr√° esta estructura:

Texto de entrada ‚Üí Preprocesamiento ‚Üí An√°lisis Completo ‚Üí Resultado
                                           ‚Üô        ‚Üò
                                    Resumen    Sentimiento


üõ†Ô∏è Implementaci√≥n Paso a Paso

1. Configuraci√≥n Inicial

Empezemos con las importaciones y configuraci√≥n b√°sica:

from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
import json
 
# Configuraci√≥n del modelo
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)


Pregunta reflexiva: ¬øPor qu√© usamos temperature=0 para an√°lisis de sentimientos?



2. Preprocesador de Texto

Objetivo: Crear una funci√≥n que limpie y prepare el texto.

Tu turno: Implementa la funci√≥n de preprocesamiento:

def preprocess_text(text):
    """Limpia el texto eliminando espacios extras y limitando longitud"""
    # Pista: usa .strip() para eliminar espacios
    # Pista: limita a 500 caracteres con slicing [:500]
    return # ¬°Completa aqu√≠!
 
# Convertir la funci√≥n en un Runnable
preprocessor = RunnableLambda(preprocess_text)


3. Generador de Res√∫menes

Objetivo: Crear una funci√≥n que genere un resumen conciso del texto.

def generate_summary(text):
    """Genera un resumen conciso del texto"""
    prompt = f"Resume en una sola oraci√≥n: {text}"
    response = llm.invoke(prompt)
    return response.content


Reto: ¬øPodr√≠as mejorar este prompt para obtener mejores res√∫menes?



4. Analizador de Sentimientos

Objetivo: Crear una funci√≥n que analice el sentimiento y devuelva JSON estructurado.

def analyze_sentiment(text):
    """Analiza el sentimiento y devuelve resultado estructurado"""
    prompt = f"""Analiza el sentimiento del siguiente texto.
    Responde √öNICAMENTE en formato JSON v√°lido:
    {{"sentimiento": "positivo|negativo|neutro", "razon": "justificaci√≥n breve"}}
    
    Texto: {text}"""
    
    response = llm.invoke(prompt)
    try:
        return json.loads(response.content)
    except json.JSONDecodeError:
        return {"sentimiento": "neutro", "razon": "Error en an√°lisis"}


Pregunta: ¬øPor qu√© es importante el manejo de errores con try/except aqu√≠?



5. Funci√≥n de Combinaci√≥n

Objetivo: Unificar los resultados de resumen y an√°lisis de sentimientos.

def merge_results(data):
    """Combina los resultados de ambas ramas en un formato unificado"""
    return {
        "resumen": data["resumen"],
        "sentimiento": data["sentimiento_data"]["sentimiento"],
        "razon": data["sentimiento_data"]["razon"]
    }


6. Funci√≥n de Procesamiento Principal

Objetivo: Coodinar el an√°lisis completo (resumen + sentimientos).

def process_one(t):
    resumen = generate_summary(t)              # Llamada 1 al LLM
    sentimiento_data = analyze_sentiment(t)    # Llamada 2 al LLM
    return merge_results({
        "resumen": resumen,
        "sentimiento_data": sentimiento_data
    })
 
# Convertir en Runnable
process = RunnableLambda(process_one)


7. Construcci√≥n de la Cadena Final

Objetivo: Conectar todos los componentes usando LCEL.

# La cadena completa
chain = preprocessor | process


¬°Eso es todo! Tu sistema est√° listo para usar.



8. Probando tu Sistema

# Prueba con diferentes textos
textos_prueba = [
    "¬°Me encanta este producto! Funciona perfectamente y lleg√≥ muy r√°pido.",
    "El servicio al cliente fue terrible, nadie me ayud√≥ con mi problema.",
    "El clima est√° nublado hoy, probablemente llueva m√°s tarde."
]
 
for texto in textos_prueba:
    resultado = chain.invoke(texto)
    print(f"Texto: {texto}")
    print(f"Resultado: {resultado}")
    print("-" * 50)


üí≠ Reflexiones Finales

¬øQu√© ventajas tiene dividir el procesamiento en funciones separadas?

¬øC√≥mo mejorar√≠as la precisi√≥n del an√°lisis de sentimientos?

¬øQu√© otros de an√°lisis podr√≠as a√±adir a este sistema?

¬øC√≥mo manejar√≠as textos en diferentes idiomas?



¬°Recuerda: el objetivo es aprender y experimentar. No te presiones para completar todo perfectamente! üöÄ

## Promt Tamplate
Son plantillas que se utilizan para generar prompts personalizados. En langchain se utilizan para definir la estructura y contenido de los mensajes que se enviar√°n al modelo de lenguaje. Es muy importante el prompt engineering para obtener mejores resultados, por que en los template promts aplicar esto determina por completo el resultado del comportamineto del modelo.

En las plantillas podemos tener roles como system, user, assistant, etc.

### Root prompts 
Los root prompts son los prompts m√°s b√°sicos que se utilizan para iniciar la conversaci√≥n con el modelo. Estos prompts se utilizan para definir la tarea que se va a realizar y los datos que se van a utilizar.

## Tarea: Mejora tu Chatbot con ChatPromptTemplate, GPT y OpenAI
üìã Objetivo

En esta tarea transformar√°s tu chatbot del tema anterior para usar ChatPromptTemplate en lugar de PromptTemplate. Descubrir√°s las ventajas de trabajar con templates dise√±ados espec√≠ficamente para modelos de chat y c√≥mo estructurar prompts de forma m√°s clara y eficiente.

‚ö†Ô∏è Importante: Esta tarea se centra en conceptos fundamentales de LangChain para aplicaciones conversacionales. Si encuentras dificultades, no te preocupes, el objetivo es que comprendas las diferencias entre ambos enfoques y sus casos de uso. La soluci√≥n completa estar√° disponible al final de este art√≠culo. ¬°Experimenta y aprende las mejores pr√°cticas!



üéØ Lo que aprender√°s

ChatPromptTemplate: Templates optimizados para modelos de chat

Estructura de mensajes: System, Human, AI messages en templates

Separaci√≥n clara: Instrucciones del sistema vs conversaci√≥n

Mejores pr√°cticas: Cu√°ndo usar cada tipo de template

Optimizaci√≥n: Aprovechamiento del formato nativo de chat de los LLMs



üèÅ Punto de partida

Tu c√≥digo actual usa PromptTemplate y deber√≠a verse as√≠:

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts import PromptTemplate
 
# ... configuraci√≥n de Streamlit ...
 
# Template actual con PromptTemplate
prompt_template = PromptTemplate(
    input_variables=["mensaje", "historial"],
    template="""Eres un asistente √∫til y amigable llamado ChatBot Pro. 
 
Historial de conversaci√≥n:
{historial}
 
Responde de manera clara y concisa a la siguiente pregunta: {mensaje}"""
)
 
# Cadena actual
cadena = prompt_template | chat_model


üîÑ ¬øPor qu√© cambiar a ChatPromptTemplate?

Problemas con PromptTemplate:

‚ùå Todo mezclado: Instrucciones del sistema y conversaci√≥n en un solo bloque de texto
‚ùå Menos claro: Dif√≠cil separar qu√© es configuraci√≥n y qu√© es conversaci√≥n
‚ùå Menos natural: No aprovecha la estructura nativa de chat de los modelos
‚ùå Mantenimiento dif√≠cil: Cambios en las instrucciones requieren editar todo el template

Ventajas de ChatPromptTemplate:

‚úÖ Estructura clara: Separaci√≥n entre system, human y assistant messages
‚úÖ Mejor organizaci√≥n: Cada tipo de mensaje tiene su prop√≥sito espec√≠fico
‚úÖ M√°s natural: Aprovecha c√≥mo est√°n entrenados los modelos de chat
‚úÖ F√°cil mantenimiento: Cambios independientes en cada secci√≥n



üõ†Ô∏è Implementaci√≥n Paso a Paso

1. Actualizar Importaciones

Primer paso: A√±adir las importaciones necesarias.

# A√±ade esta importaci√≥n a las existentes
from langchain.prompts import ChatPromptTemplate


2. Crear el ChatPromptTemplate

Objetivo: Reemplazar el PromptTemplate actual con ChatPromptTemplate.

Tu turno: Reemplaza el template actual con este nuevo enfoque:

# Reemplaza el PromptTemplate existente con:
chat_prompt = ChatPromptTemplate.from_messages([
    # Mensaje del sistema - Define la personalidad una sola vez
    ("system", "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa."),
    
    # El historial y mensaje actual - se manejan como texto formateado
    ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}")
])


Diferencias clave:

("system", "..."): Define el comportamiento base del asistente (separado y claro)

("human", "..."): Contiene el historial y la pregunta actual

Estructura clara: Cada mensaje tiene un rol espec√≠fico



3. Actualizar la Cadena

Objetivo: Usar el nuevo template en la cadena.

# Actualiza la cadena (¬°sigue siendo simple!)
cadena = chat_prompt | chat_model


4. Personalizaci√≥n del Sistema (Desaf√≠o)

Objetivo: Hacer el mensaje del sistema configurable desde el sidebar.

with st.sidebar:
    st.header("Configuraci√≥n")
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.5, 0.1)
    model_name = st.selectbox("Modelo", ["gpt-3.5-turbo", "gpt-4", "gpt-4o-mini"])
    
    # ¬°Nuevo! Personalidad configurable
    personalidad = st.selectbox(
        "Personalidad del Asistente",
        [
            "√ötil y amigable",
            "Profesional y formal", 
            "Casual y relajado",
            "Experto t√©cnico",
            "Creativo y divertido"
        ]
    )
    
    # Recrear modelo
    chat_model = ChatOpenAI(model=model_name, temperature=temperature)
    
    # Template din√°mico basado en personalidad
    system_messages = {
        "√ötil y amigable": "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa.",
        "Profesional y formal": "Eres un asistente profesional y formal. Proporciona respuestas precisas y bien estructuradas.",
        "Casual y relajado": "Eres un asistente casual y relajado. Habla de forma natural y amigable, como un buen amigo.",
        "Experto t√©cnico": "Eres un asistente experto t√©cnico. Proporciona respuestas detalladas con precisi√≥n t√©cnica.",
        "Creativo y divertido": "Eres un asistente creativo y divertido. Usa analog√≠as, ejemplos creativos y mant√©n un tono alegre."
    }
    
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", system_messages[personalidad]),
        ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}")
    ])
    
    cadena = chat_prompt | chat_model


üí° Conceptos Clave a Recordar

Estructura de ChatPromptTemplate

ChatPromptTemplate.from_messages([
    ("system", "Instrucciones base del asistente"),    # Configuraci√≥n
    ("human", "Contenido del usuario + historial"),   # Datos de entrada
    ("assistant", "Respuesta anterior (opcional)")    # Para few-shot examples
])


Roles de mensajes:

system: Instrucciones y configuraci√≥n del comportamiento

human: Mensajes del usuario (incluyendo contexto/historial)

assistant: Respuestas del modelo (para ejemplos o continuaci√≥n)

Ventajas del nuevo enfoque:

Claridad: Separaci√≥n visual entre configuraci√≥n y datos

Mantenimiento: Cambios independientes en cada secci√≥n

Flexibilidad: F√°cil intercambiar instrucciones del sistema

Mejores pr√°cticas: Sigue las convenciones de modelos de chat



üìù Soluci√≥n Completa

Aqu√≠ tienes el c√≥digo completo con la transformaci√≥n de PromptTemplate a ChatPromptTemplate, incluyendo el desaf√≠o del punto 4 (personalizaci√≥n del sistema):

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate
 
# Configuraci√≥n inicial
st.set_page_config(page_title="Chatbot B√°sico", page_icon="ü§ñ")
st.title("ü§ñ Chatbot B√°sico con LangChain")
st.markdown("Este es un *chatbot de ejemplo* construido con LangChain + Streamlit. ¬°Escribe tu mensaje abajo para comenzar!")
 
with st.sidebar:
    st.header("Configuraci√≥n")
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.5, 0.1)
    model_name = st.selectbox("Modelo", ["gpt-3.5-turbo", "gpt-4", "gpt-4o-mini"])
    
    # PUNTO 4: Personalidad configurable
    personalidad = st.selectbox(
        "Personalidad del Asistente",
        [
            "√ötil y amigable",
            "Profesional y formal", 
            "Casual y relajado",
            "Experto t√©cnico",
            "Creativo y divertido"
        ]
    )
    
    # Recrear el modelo con nuevos par√°metros
    chat_model = ChatOpenAI(model=model_name, temperature=temperature)
    
    # Definir mensajes del sistema seg√∫n personalidad
    system_messages = {
        "√ötil y amigable": "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa.",
        "Profesional y formal": "Eres un asistente profesional y formal. Proporciona respuestas precisas y bien estructuradas.",
        "Casual y relajado": "Eres un asistente casual y relajado. Habla de forma natural y amigable, como un buen amigo.",
        "Experto t√©cnico": "Eres un asistente experto t√©cnico. Proporciona respuestas detalladas con precisi√≥n t√©cnica.",
        "Creativo y divertido": "Eres un asistente creativo y divertido. Usa analog√≠as, ejemplos creativos y mant√©n un tono alegre."
    }
    
    # NUEVO: ChatPromptTemplate con personalidad din√°mica
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", system_messages[personalidad]),
        ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}")
    ])
    
    # Crear cadena usando LCEL
    cadena = chat_prompt | chat_model
 
# Inicializar el historial de mensajes en session_state
if "mensajes" not in st.session_state:
    st.session_state.mensajes = []
 
# Renderizar historial existente
for msg in st.session_state.mensajes:
    if isinstance(msg, SystemMessage):
        continue  # no mostrar mensajes del sistema al usuario
    
    role = "assistant" if isinstance(msg, AIMessage) else "user"
    with st.chat_message(role):
        st.markdown(msg.content)
 
if st.button("üóëÔ∏è Nueva conversaci√≥n"):
    st.session_state.mensajes = []
    st.rerun()
 
# Input de usuario
pregunta = st.chat_input("Escribe tu mensaje:")
 
if pregunta:
    # Mostrar y almacenar mensaje del usuario
    with st.chat_message("user"):
        st.markdown(pregunta)
    
    # Preparar historial como texto
    historial_texto = ""
    for msg in st.session_state.mensajes[-10:]:  # √öltimos 10 mensajes
        if isinstance(msg, HumanMessage):
            historial_texto += f"Usuario: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            historial_texto += f"Asistente: {msg.content}\n"
    
    if not historial_texto:
        historial_texto = "(No hay historial previo)"
    
    # Generar y mostrar respuesta del asistente
    try:
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
 
            # Streaming de la respuesta con ChatPromptTemplate
            for chunk in cadena.stream({"mensaje": pregunta, "historial": historial_texto}):
                full_response += chunk.content
                response_placeholder.markdown(full_response + "‚ñå")
            
            response_placeholder.markdown(full_response)
        
        # Almacenar mensajes en el historial
        st.session_state.mensajes.append(HumanMessage(content=pregunta))
        st.session_state.mensajes.append(AIMessage(content=full_response))
        
    except Exception as e:
        st.error(f"Error al generar respuesta: {str(e)}")
        st.info("Verifica que tu API Key de OpenAI est√© configurada correctamente.")


üîë Puntos Clave de la Soluci√≥n

1. La diferencia principal - PromptTemplate vs ChatPromptTemplate:

# ‚ùå ANTES: PromptTemplate (todo mezclado)
prompt_template = PromptTemplate(
    input_variables=["mensaje", "historial"],
    template="""Eres un asistente √∫til y amigable llamado ChatBot Pro. 
 
Historial de conversaci√≥n:
{historial}
 
Responde de manera clara y concisa a la siguiente pregunta: {mensaje}"""
)
 
# ‚úÖ AHORA: ChatPromptTemplate (estructura clara)
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa."),
    ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}")
])


2. Ventajas implementadas:

‚úÖ Mejor estructura: System message separado del human message

‚úÖ M√°s claro: Instrucciones del asistente vs datos del usuario bien diferenciados

‚úÖ Mejor pr√°ctica: Sigue las convenciones de modelos de chat

‚úÖ F√°cil mantenimiento: Cambios independientes en system vs human messages



3. El uso sigue siendo id√©ntico:

# La cadena funciona exactamente igual
cadena = chat_prompt | chat_model
cadena.stream({"mensaje": pregunta, "historial": historial_texto})


4. Preparaci√≥n del historial:

# El historial se sigue formateando como texto
historial_texto = ""
for msg in st.session_state.mensajes[-10:]:
    if isinstance(msg, HumanMessage):
        historial_texto += f"Usuario: {msg.content}\n"
    elif isinstance(msg, AIMessage):
        historial_texto += f"Asistente: {msg.content}\n"


¬°La mejora es sutil pero importante: mejor organizaci√≥n y estructura m√°s profesional! üéâ


### Lectura: MessagesPlaceholder y Few-Shot Examples
¬øQu√© es el In-Context Learning (ICL)?

El In-Context Learning o Aprendizaje en Contexto es la capacidad de los modelos de lenguaje de aprender nuevas tareas o patrones simplemente proporcion√°ndoles ejemplos dentro del mismo prompt, sin necesidad de entrenar o ajustar el modelo.

Es como mostrarle a alguien c√≥mo hacer algo d√°ndole ejemplos directos:

"Mira, as√≠ se hace esto... y esto otro... ¬øahora puedes hacer t√∫ algo similar?"



¬øQu√© son los Few-Shot Examples?

Los Few-Shot Examples (ejemplos de pocos intentos) son una t√©cnica de ICL donde proporcionamos al modelo entre 2-10 ejemplos de la tarea que queremos que realice. Estos ejemplos sirven como "entrenamiento instant√°neo".

Estructura t√≠pica:

Sistema: Instrucciones generales
Ejemplo 1: Input ‚Üí Output esperado
Ejemplo 2: Input ‚Üí Output esperado  
Ejemplo 3: Input ‚Üí Output esperado
Pregunta real: Input actual ‚Üí ?


MessagesPlaceholder: La Herramienta Perfecta

MessagesPlaceholder es ideal para few-shot examples porque:

‚úÖ Mantiene la estructura de mensajes (Human/AI)

‚úÖ Permite insertar m√∫ltiples ejemplos din√°micamente

‚úÖ El modelo entiende mejor el formato de conversaci√≥n

‚úÖ F√°cil de reutilizar para diferentes tareas



Evoluci√≥n del C√≥digo: De Historial a Few-Shot

C√≥digo Base (Historial de Conversaci√≥n)

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
 
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente √∫til que mantiene el contexto de la conversaci√≥n."),
    MessagesPlaceholder(variable_name="historial"),
    ("human", "Usuario: {pregunta_actual}")
])
 
# Simulamos un historial de conversaci√≥n
historial_conversacion = [
    HumanMessage(content="Usuario: ¬øCu√°l es la capital de Francia?"),
    AIMessage(content="IA: La capital de Francia es Par√≠s."),
    HumanMessage(content="Usuario: ¬øY cu√°ntos habitantes tiene?"),
    AIMessage(content="IA: Par√≠s tiene aproximadamente 2.2 millones de habitantes en la ciudad propiamente dicha.")
]


C√≥digo Evolucionado (Few-Shot Examples)

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
 
# Template para clasificaci√≥n de sentimientos con few-shot examples
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un experto en an√°lisis de sentimientos. Clasifica cada texto como: POSITIVO, NEGATIVO o NEUTRO."),
    MessagesPlaceholder(variable_name="ejemplos"),
    ("human", "Texto a analizar: {texto_usuario}")
])
 
# Few-shot examples para an√°lisis de sentimientos
ejemplos_sentimientos = [
    HumanMessage(content="Texto a analizar: Me encanta este producto, es incre√≠ble"),
    AIMessage(content="POSITIVO"),
    HumanMessage(content="Texto a analizar: El servicio fue terrible, muy decepcionante"),
    AIMessage(content="NEGATIVO"),
    HumanMessage(content="Texto a analizar: El clima est√° nublado hoy"),
    AIMessage(content="NEUTRO")
]
 
# Generar el prompt con los ejemplos
mensajes = chat_prompt.format_messages(
    ejemplos=ejemplos_sentimientos,
    texto_usuario="¬°Qu√© d√≠a tan maravilloso!"
)
 
# Ver el resultado
for i, m in enumerate(mensajes):
    print(f"Mensaje {i+1} ({m.__class__.__name__}):")
    print(m.content)
    print("-" * 40)


Salida del C√≥digo:

Mensaje 1 (SystemMessage):
Eres un experto en an√°lisis de sentimientos. Clasifica cada texto como: POSITIVO, NEGATIVO o NEUTRO.
----------------------------------------
Mensaje 2 (HumanMessage):
Texto a analizar: Me encanta este producto, es incre√≠ble
----------------------------------------
Mensaje 3 (AIMessage):
POSITIVO
----------------------------------------
Mensaje 4 (HumanMessage):
Texto a analizar: El servicio fue terrible, muy decepcionante
----------------------------------------
Mensaje 5 (AIMessage):
NEGATIVO
----------------------------------------
Mensaje 6 (HumanMessage):
Texto a analizar: El clima est√° nublado hoy
----------------------------------------
Mensaje 7 (AIMessage):
NEUTRO
----------------------------------------
Mensaje 8 (HumanMessage):
Texto a analizar: ¬°Qu√© d√≠a tan maravilloso!
----------------------------------------


Ventajas de Este Enfoque

Con MessagesPlaceholder:

‚úÖ Estructura clara: Cada ejemplo mantiene su rol (Human/AI)
‚úÖ Escalable: F√°cil a√±adir/quitar ejemplos
‚úÖ Reutilizable: Cambiar ejemplos = Nueva tarea
‚úÖ Natural: El modelo entiende el formato conversacional

Sin MessagesPlaceholder (texto plano):

‚ùå Menos claro: Todo mezclado en un string
‚ùå M√°s errores: El modelo puede confundirse
‚ùå Dif√≠cil mantenimiento: Cambios requieren reescribir todo
‚ùå Menos efectivo: Pierde la estructura conversacional



Otros Ejemplos de Uso

1. Extracci√≥n de Informaci√≥n

ejemplos_extraccion = [
    HumanMessage(content="Texto: Juan P√©rez trabaja en Google como ingeniero desde 2020"),
    AIMessage(content="Nombre: Juan P√©rez, Empresa: Google, Puesto: ingeniero, A√±o: 2020"),
    HumanMessage(content="Texto: Mar√≠a Silva es doctora en el Hospital Central"),
    AIMessage(content="Nombre: Mar√≠a Silva, Empresa: Hospital Central, Puesto: doctora, A√±o: N/A")
]


2. Traducci√≥n con Estilo

ejemplos_traduccion = [
    HumanMessage(content="Formal en ingl√©s: Good morning, how are you today?"),
    AIMessage(content="Casual en espa√±ol: ¬°Hola! ¬øQu√© tal?"),
    HumanMessage(content="Formal en ingl√©s: I would like to schedule a meeting"),
    AIMessage(content="Casual en espa√±ol: ¬øPodemos quedar?")
]


Mejores Pr√°cticas

2-5 ejemplos: Suficiente para mostrar el patr√≥n, no demasiados

Ejemplos diversos: Cubrir diferentes casos/variaciones

Formato consistente: Mismo patr√≥n en todos los ejemplos

Ejemplos de calidad: Outputs perfectos como referencia



Conclusi√≥n

MessagesPlaceholder transforma los few-shot examples de una t√©cnica b√°sica a una herramienta poderosa y flexible. La estructura clara de mensajes Human/AI hace que el modelo comprenda mejor qu√© esperamos, resultando en respuestas m√°s precisas y consistentes.

Recuerda: No es solo insertar ejemplos, es ense√±arle al modelo a trav√©s de la conversaci√≥n. ¬°Esa es la magia del In-Context Learning!

## Pydantic
Es una biblioteca de Python para la validaci√≥n de datos y la serializaci√≥n de objetos. Permite definir esquemas de datos y validar los datos de entrada de manera sencilla.

