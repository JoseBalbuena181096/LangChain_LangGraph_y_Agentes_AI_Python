leto: LangChain, LangGraph y Agentes IA con Python
Imagen del curso
99. Tipos de memoria principales con LangGraph
Reproducir
94. Introducción a la sección
1 min
Reproducir
95. Fundamentos de Memoria en LangChain y LangGraph
10 min
Reproducir
96. Memoria en LangChain: RunnableWithMessageHistory
12 min
Reproducir
97. Memoria en LangGraph: MemorySaver
17 min
Reproducir
98. Memoria con ventana deslizante en LangGraph
11 min
Iniciar
99. Tipos de memoria principales con LangGraph
5 min
Reproducir
100. Memoria persistente con LangGraph
8 min
Reproducir
101. Memoria Vectorial con LangGraph
3 min
Reproducir
102. Caso practico: Memoria Vectorial con LangGraph
30 min
Reproducir
103. Proyecto: Chat Multi-Usuario con Memoria Avanzada
6 min
Iniciar
104. Proyecto: Descarga los archivos del proyecto
1 min
Reproducir
105. Proyecto: Configuración de la aplicación
6 min
Reproducir
106. Proyecto: Definición del estado extendido y un modelo de Pydantic
11 min
Reproducir
107. Proyecto: Definicion de la clase de gestion de memoria
5 min
Reproducir
108. Proyecto: Inicializar la base de datos vectorial
5 min
Reproducir
109. Proyecto: Sistema de extracción inteligente
7 min
Reproducir
110. Proyecto: Gestión de múltiples Chats - Parte 1
11 min
Reproducir
111. Proyecto: Gestión de múltiples Chats - Parte 2
13 min
Reproducir
112. Proyecto: Implementación de la memoria vectorial
10 min
Reproducir
113. Proyecto: Implementación de la extracción inteligente
6 min
Reproducir
114. Proyecto: Gestión de los usuarios de la aplicación
5 min
Reproducir
115. Proyecto: Inicializar el Chatbot avanzado
8 min
Reproducir
116. Proyecto: Implementación del Chatbot y grafo de LangGraph
20 min
Reproducir
117. Proyecto: Implementación del chat de la aplicación
11 min
Reproducir
118. Proyecto: Obtener el historial de una conversación
8 min
Reproducir
119. Proyecto: Gestión de Chatbots
4 min
Reproducir
120. Proyecto: Interfaz y ejecución de la aplicación
10 min

Crea apps profesionales con LLMs, Inteligencia Artificial Generativa y Agentes de IA con LangChain, LangGraph y Python
Calificación: 4,8 de 5
4,8
125 calificaciones
1077
Estudiantes
18 horas
Total
Última actualización noviembre de 2025
Español
Español [automático]
alerta informativa
Programa un tiempo de aprendizaje
Aprender un poco cada día marca la diferencia. Hay estudios que muestran que los estudiantes que hacen del aprendizaje un hábito tienen una mayor probabilidad de alcanzar sus objetivos. Reserva tiempo para aprender y recibe recordatorios con la herramienta de planificación del aprendizaje.
Por cifras
Nivel de habilidad: Todos los niveles
Estudiantes: 1077
Idiomas: Español
Subtítulos: Sí
clases: 149
Vídeo: 18 horas en total
Certificados
Consigue el certificado de Udemy al completar todo el curso

Características
Disponible en iOS y Android
Descripción
En este curso 100% práctico y desde cero, dominarás las tecnologías más avanzadas para crear agentes de Inteligencia Artificial profesionales usando Python, LangChain y LangGraph. Aprenderás a construir sistemas de IA que procesan información, toman decisiones autónomas y resuelven problemas complejos del mundo real.



¿Qué lograrás con este curso?

Al finalizar, serás capaz de desarrollar sistemas completos de Inteligencia Artificial que integran múltiples fuentes de datos, mantienen memoria persistente y ejecutan tareas complejas de forma autónoma. Podrás crear desde chatbots avanzados hasta sistemas multi-agente especializados para casos de uso empresariales.

Imagina construir un asistente legal que analiza contratos automáticamente, un sistema de atención al cliente que escala a humanos cuando es necesario, o un centro de operaciones de seguridad con múltiples agentes especializados, todo esto lo desarrollarás durante el curso.



Lo que aprenderás:

Fundamentos sólidos de LangChain: Comenzarás desde cero con los conceptos esenciales. Dominarás la arquitectura de LangChain, incluyendo Runnables, LCEL (LangChain Expression Language), Prompt Templates y Output Parsers. Aprenderás a estructurar respuestas con Pydantic y crear cadenas de procesamiento robustas.

Sistemas RAG (Retrieval Augmented Generation): Construirás sistemas que conectan IA generativa con datos del mundo real. Aprenderás Document Loaders, Text Splitters, Embeddings, bases de datos vectoriales y estrategias avanzadas de recuperación. Crearás sistemas que pueden procesar documentos, PDFs y fuentes externas de información.

Dominio completo de LangGraph: Utilizarás la tecnología más avanzada para crear flujos de trabajo inteligentes. Aprenderás control de flujo, toma de decisiones, estados avanzados, checkpoints y persistencia. Implementarás sistemas que pueden pausarse, reanudarse y mantener contexto a largo plazo.

Gestión avanzada de memoria: Implementarás diferentes tipos de memoria en tus agentes: memoria de conversación, memoria vectorial, ventanas deslizantes y memoria persistente. Crearás sistemas que recuerdan interacciones pasadas y aprenden de ellas, incluyendo chat multi-usuario con memoria independiente.

Herramientas y Agentes de IA especializados: Integrarás APIs externas, herramientas personalizadas y servicios web. Construirás agentes que pueden buscar en internet, analizar ciberseguridad, procesar reuniones y coordinar tareas complejas. Aprenderás tanto agentes de IA individuales como sistemas multi-agente coordinados.

Aplicaciones web completas: Desarrollarás interfaces profesionales con Streamlit y APIs con FastAPI. Crearás dashboards interactivos, sistemas de evaluación de CVs, asistentes legales y centros de operaciones completos con interfaces web funcionales.

Casos de uso empresariales reales: Cada módulo incluye proyectos prácticos basados en necesidades reales del mercado: sistemas de evaluación de candidatos, asistentes legales para análisis de contratos, helpdesks inteligentes con escalado humano, y centros de operaciones de seguridad multi-agente.



Por qué este curso es para ti:

Si eres desarrollador, científico de datos, o profesional técnico que quiere dominar la Inteligencia Artificial aplicada, este curso te dará las habilidades más demandadas del mercado. No necesitas experiencia previa en IA, pero sí conocimientos básicos de Python.

Cada lección combina teoría esencial con implementación práctica inmediata. Terminarás cada sección con proyectos funcionales que podrás usar en tu portafolio profesional. El curso está diseñado para llevarte desde los conceptos básicos hasta implementaciones empresariales avanzadas.



Tecnologías que dominarás:

LangChain y LangGraph (último stack de IA)

OpenAI, Google Gemini, y modelos open source

Bases de datos vectoriales (ChromaDB, FAISS)

APIs modernas (FastAPI, REST)

Streamlit para interfaces web

Herramientas especializadas (Tavily Search, GmailToolkit)



Transformación garantizada:

Al completar este curso, tendrás las habilidades para desarrollar cualquier sistema de IA empresarial. Podrás identificar oportunidades de automatización inteligente, diseñar arquitecturas robustas y implementar soluciones escalables que generen valor real en organizaciones.

Saldrás con conocimientos profundos en las tecnologías más avanzadas de IA, un portafolio de proyectos profesionales y la confianza para liderar iniciativas de Inteligencia Artificial en tu empresa o como freelancer.



¡No te quedes atrás en la revolución de la IA generativa! Únete ahora y conviértete en un experto en las tecnologías que están definiendo el futuro de la automatización inteligente.

¡Te esperamos dentro del curso para construir juntos el futuro de la Inteligencia Artificial!

Lo que aprenderás
Crear agentes de IA profesionales desde cero usando LangChain, LangGraph y Python para automatizar procesos empresariales complejos.
Construir sistemas de IA con RAG avanzado que integran documentos PDF, bases de datos y fuentes externas para crear asistentes expertos en cualquier dominio.
Construir sistemas multi-agente coordinados con agentes especializados que colaboran para resolver problemas complejos.
Desarrollar chatbots con memoria inteligente que recuerdan conversaciones pasadas, aprenden de interacciones y mantienen contexto en múltiples sesiones.
Implementar agentes multi-herramienta que buscan en internet, procesan documentos y toman decisiones autónomas usando APIs externas.
Diseñar workflows inteligentes con LangGraph que incluyen escalado humano, checkpoints, pausas/reanudación y control de flujo avanzado.
Integrar múltiples LLMs y herramientas: OpenAI, Google Gemini, Tavily Search y herramientas personalizadas en un solo sistema cohesivo.
Implementar bases de datos vectoriales (ChromaDB, FAISS) con estrategias de recuperación híbrida para búsquedas semánticas ultra-precisas.
Aplicar técnicas de Prompt Engineering avanzado con plantillas dinámicas, few-shot learning, roles específicos y estructuración de respuestas con Pydantic.
Desarrollar sistemas empresariales reales: helpdesks inteligentes, asistentes legales, procesadores de reuniones y evaluadores automáticos de candidatos.
¿Hay requisitos para realizar el curso?
Conocimientos básicos de Python: No necesitas ser un experto, pero sí tener unas nociones básicas de este lenguaje de programación.
Una computadora, conexión a Internet y muchas ganas de aprender y experimentar con las tecnologías más avanzadas de IA de la actualidad.
¿Para quién es este curso?
Para profesionales técnicos que buscan dominar LangChain y LangGraph para crear soluciones empresariales de Inteligencia Artificial.
Para científicos de datos y analistas que quieren expandir sus habilidades hacia la IA generativa y sistemas de agentes inteligentes.
Para desarrolladores Python que quieren especializarse en las tecnologías de IA más demandadas del mercado actual.
Para estudiantes de ingeniería, informática o carreras técnicas que desean destacar con las competencias más valiosas del futuro tecnológico.
Para freelancers y consultores que quieren ofrecer servicios de desarrollo de agentes de IA y sistemas RAG a empresas y startups.
Para emprendedores con perfil técnico que buscan construir productos innovadores basados en IA sin depender completamente de equipos externos.
Para profesionales de TI y arquitectos de software que necesitan implementar soluciones de IA escalables en entornos empresariales.
Para entusiastas de la programación que quieren entender a profundidad cómo funcionan los sistemas de IA más avanzados y crear sus propias implementaciones.
Para equipos de desarrollo, DevOps y ingeniería que buscan automatizar procesos complejos con agentes inteligentes y sistemas de toma de decisiones.
Instructor

Santiago Hernández
Experto en Ciberseguridad e Inteligencia Artificial

Santiago Hernández es ingeniero informático por la Universidad de Salamanca, con un máster en seguridad de la información y de las comunicaciones por la Universidad Europea y doctorando en seguridad de la información e inteligencia artificial. Su experiencia profesional abarca diferentes posiciones en el ámbito de la Ciberseguridad y de la la Inteligencia Artificial en compañías como Endesa, Telefónica/11paths o BBVA. Además, es profesor universitario en la UEM, en la UCLM y UCAM en diferentes postgrados relacionados con la Inteligencia Artificial y la seguridad de la información y ponente habitual en conferencias especializadas. Entre las conferencias en las que ha participado como ponente se encuentran: BlackHat Europe, ToorCon San Diego, Navaja Negra, Noconname, Cybercamp, CCN-CERT…

---

Santiago Hernández is a computer engineer from the University of Salamanca, holding a master's degree in Information and Communications Security from the European University, and currently pursuing a Ph.D. in Information Security and Artificial Intelligence. His professional experience spans various roles in the fields of Cybersecurity and Artificial Intelligence at companies such as Endesa, Telefónica/11paths, and BBVA. Additionally, he is a university lecturer at UEM, UCLM, and UCAM in several postgraduate programs related to Artificial Intelligence and Information Security, and a frequent speaker at specialized conferences. Among the conferences where he has participated as a speaker are BlackHat Europe, ToorCon San Diego, Navaja Negra, Noconname, Cybercamp, CCN-CERT, and others.















Enseña al mundo en línea
Crea un curso en vídeo en línea, llega a estudiantes de todo el mundo y gana dinero
Las principales empresas eligen a Udemy Business para desarrollar sus habilidades profesionales más demandadas.
NasdaqVolkswagenNetAppEventbrite
Udemy Business
Enseña en Udemy
Consigue la aplicación
¿Quiénes somos?
Ponte en contacto con nosotros
Empleo
Blog
Ayuda y asistencia
Afiliado
Inversores
Condiciones
Política de Privacidad
Mapa del sitio
Declaración de accesibilidad
© 2025 Udemy, Inc.
99. Tipos de memoria principales con LangGraph
Tipos de memoria principales con LangGraph
La gestión inteligente de memoria es fundamental para crear agentes conversacionales efectivos. Mientras que la memoria básica y la ventana deslizante cubren casos fundamentales, existen estrategias más sofisticadas que pueden transformar significativamente la experiencia del usuario. En este artículo, exploraremos las técnicas de gestión de memoria más importantes que puedes implementar en LangGraph.

El Problema de la Memoria Ilimitada
Antes de explorar las soluciones, es crucial entender por qué necesitamos estrategias de gestión de memoria:

Limitaciones de contexto: Los LLMs tienen ventanas de contexto finitas

Costos crecientes: Más tokens significan mayor costo por llamada

Degradación de rendimiento: Contextos muy largos pueden distraer al modelo

Latencia: Procesar historiales extensos aumenta el tiempo de respuesta

Estrategias de Gestión de Memoria
1. Memoria de Resumen (Summarization Memory)
Esta estrategia condensa automáticamente conversaciones largas en resúmenes concisos, preservando el contexto esencial mientras mantiene el historial manejable.

Cuándo usar: Conversaciones largas donde el contexto histórico es importante pero no necesitas cada detalle.

```python
from typing import TypedDict, List
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
 
class ChatState(TypedDict):
    messages: List[BaseMessage]
    conversation_summary: str
    message_count: int
 
def summarize_conversation(state: ChatState) -> dict:
    """Condensa mensajes antiguos en un resumen cuando se alcanza el límite"""
    messages = state["messages"]
    current_summary = state.get("conversation_summary", "")
    
    # Si tenemos menos de 10 mensajes, no resumir aún
    if len(messages) < 10:
        return {"message_count": len(messages)}
    
    # Mantener los últimos 4 mensajes y resumir el resto
    recent_messages = messages[-4:]
    messages_to_summarize = messages[:-4]
    
    # Crear prompt para resumir
    summary_prompt = f"""
    Resumen anterior: {current_summary}
    
    Nuevos mensajes a resumir:
    {[f"{msg.type}: {msg.content}" for msg in messages_to_summarize]}
    
    Crea un resumen conciso que capture los puntos clave de la conversación.
    """
    
    # Aquí llamarías a tu LLM para generar el resumen
    new_summary = "Resumen actualizado de la conversación..."  # Placeholder
    
    # Crear mensaje de sistema con el resumen
    summary_message = SystemMessage(content=f"Resumen de conversación previa: {new_summary}")
    
    return {
        "messages": [summary_message] + recent_messages,
        "conversation_summary": new_summary,
        "message_count": len(recent_messages) + 1
    }
 
def should_summarize(state: ChatState) -> str:
    """Decide si necesitamos resumir la conversación"""
    if len(state["messages"]) >= 10:
        return "summarize"
    return "continue"
```


2. Memoria de Filtrado Inteligente
En lugar de eliminar mensajes por antigüedad, esta estrategia mantiene los mensajes más relevantes o importantes según criterios específicos.

Cuándo usar: Cuando algunos mensajes son más valiosos que otros (decisiones importantes, preferencias del usuario, etc.).

```python
def filter_important_messages(state: ChatState) -> dict:
    """Mantiene mensajes importantes y elimina menos relevantes"""
    messages = state["messages"]
    
    if len(messages) <= 8:
        return {}  # No necesita filtrado aún
    
    important_messages = []
    regular_messages = []
    
    for msg in messages:
        # Criterios para mensajes importantes
        is_important = (
            isinstance(msg, SystemMessage) or  # Siempre mantener system messages
            "importante" in msg.content.lower() or
            "recuerda" in msg.content.lower() or
            "preferencia" in msg.content.lower() or
            len(msg.content) > 200  # Mensajes largos pueden ser importantes
        )
        
        if is_important:
            important_messages.append(msg)
        else:
            regular_messages.append(msg)
    
    # Mantener todos los importantes + los 4 regulares más recientes
    filtered_messages = important_messages + regular_messages[-4:]
    
    return {"messages": filtered_messages}
 
def analyze_message_importance(state: ChatState) -> str:
    """Decide si aplicar filtrado por importancia"""
    if len(state["messages"]) > 8:
        return "filter"
    return "continue"
```


3. Memoria por Límite de Tokens
Gestiona la memoria basándose en el conteo real de tokens, proporcionando control preciso sobre el uso del contexto.

Cuándo usar: Cuando necesitas optimización precisa de costos o trabajas cerca de límites de contexto específicos.

```python
def estimate_tokens(text: str) -> int:
    """Estimación simple de tokens (aproximadamente 4 caracteres por token)"""
    return len(text) // 4
 
def manage_memory_by_tokens(state: ChatState, max_tokens: int = 2000) -> dict:
    """Gestiona memoria basándose en límite de tokens"""
    messages = state["messages"]
    
    # Calcular tokens actuales
    total_tokens = sum(estimate_tokens(msg.content) for msg in messages)
    
    if total_tokens <= max_tokens:
        return {}  # No necesita gestión
    
    # Estrategia: mantener primer mensaje (system) + mensajes más recientes
    if messages and isinstance(messages[0], SystemMessage):
        system_msg = messages[0]
        other_messages = messages[1:]
        current_tokens = estimate_tokens(system_msg.content)
    else:
        system_msg = None
        other_messages = messages
        current_tokens = 0
    
    # Agregar mensajes desde el más reciente hasta alcanzar el límite
    selected_messages = []
    for msg in reversed(other_messages):
        msg_tokens = estimate_tokens(msg.content)
        if current_tokens + msg_tokens <= max_tokens:
            selected_messages.insert(0, msg)
            current_tokens += msg_tokens
        else:
            break
    
    # Reconstruir lista de mensajes
    final_messages = []
    if system_msg:
        final_messages.append(system_msg)
    final_messages.extend(selected_messages)
    
    return {"messages": final_messages}
 
def check_token_limit(state: ChatState) -> str:
    """Verifica si necesitamos gestión por tokens"""
    total_tokens = sum(estimate_tokens(msg.content) for msg in state["messages"])
    if total_tokens > 2000:
        return "manage_tokens"
    return "continue"
```


4. Memoria Híbrida por Tipo de Mensaje
Aplica diferentes estrategias de retención según el tipo de mensaje, optimizando para diferentes patrones de uso.

Cuándo usar: En agentes que manejan diferentes tipos de interacciones (comandos, chat casual, tareas específicas).

```python
def hybrid_memory_management(state: ChatState) -> dict:
    """Aplica diferentes estrategias según el tipo de mensaje"""
    messages = state["messages"]
    
    if len(messages) <= 6:
        return {}
    
    system_messages = []
    human_messages = []
    ai_messages = []
    
    # Clasificar mensajes por tipo
    for msg in messages:
        if isinstance(msg, SystemMessage):
            system_messages.append(msg)
        elif isinstance(msg, HumanMessage):
            human_messages.append(msg)
        elif isinstance(msg, AIMessage):
            ai_messages.append(msg)
    
    # Estrategias diferenciadas:
    # - Mantener TODOS los system messages
    # - Mantener los últimos 4 human messages
    # - Mantener solo las últimas 2 AI responses
    
    filtered_messages = []
    filtered_messages.extend(system_messages)  # Todos los system
    filtered_messages.extend(human_messages[-4:])  # Últimos 4 human
    filtered_messages.extend(ai_messages[-2:])  # Últimas 2 AI
    
    # Reordenar cronológicamente
    # (En implementación real, mantendrías timestamps)
    filtered_messages.sort(key=lambda x: messages.index(x))
    
    return {"messages": filtered_messages}
```


5. Memoria con Ventana Deslizante Adaptativa
Extiende la ventana deslizante básica con lógica adaptativa que ajusta el tamaño de la ventana según el contexto.

Cuándo usar: Cuando la importancia del contexto varía según el tipo de conversación o tarea.

```python
def adaptive_sliding_window(state: ChatState) -> dict:
    """Ventana deslizante que se adapta al contexto"""
    messages = state["messages"]
    
    # Determinar tamaño de ventana basado en el contexto
    window_size = calculate_adaptive_window_size(state)
    
    if len(messages) <= window_size:
        return {}
    
    # Mantener los mensajes más recientes
    recent_messages = messages[-window_size:]
    
    return {"messages": recent_messages}
 
def calculate_adaptive_window_size(state: ChatState) -> int:
    """Calcula tamaño de ventana dinámicamente"""
    messages = state["messages"]
    
    # Ventana base
    base_size = 6
    
    # Ajustes según patrones detectados
    if any("código" in msg.content.lower() for msg in messages[-3:]):
        return base_size + 4  # Más contexto para programación
    
    if any("historia" in msg.content.lower() for msg in messages[-2:]):
        return base_size + 6  # Más contexto para narrativas
    
    if any(len(msg.content) > 500 for msg in messages[-2:]):
        return base_size - 2  # Menos mensajes si son muy largos
    
    return base_size
 
def determine_window_strategy(state: ChatState) -> str:
    """Decide qué estrategia de ventana usar"""
    if len(state["messages"]) > calculate_adaptive_window_size(state):
        return "adaptive_window"
    return "continue"
```


6. Memoria con Prioridad de Contexto
Mantiene mensajes basándose en su relevancia para la conversación actual, usando análisis semántico simple.

Cuándo usar: Conversaciones que saltan entre temas pero donde el contexto temático es crucial.

```python
def priority_context_memory(state: ChatState) -> dict:
    """Mantiene mensajes relevantes al contexto actual"""
    messages = state["messages"]
    
    if len(messages) <= 8:
        return {}
    
    # Obtener temas de los últimos mensajes
    recent_content = " ".join([msg.content for msg in messages[-3:]])
    current_keywords = extract_keywords(recent_content)
    
    # Puntuar mensajes por relevancia
    scored_messages = []
    for i, msg in enumerate(messages):
        relevance_score = calculate_relevance(msg.content, current_keywords)
        # Los mensajes más recientes tienen bonus
        recency_bonus = max(0, len(messages) - i) * 0.1
        total_score = relevance_score + recency_bonus
        scored_messages.append((total_score, msg))
    
    # Mantener top 8 mensajes más relevantes
    scored_messages.sort(reverse=True)
    selected_messages = [msg for _, msg in scored_messages[:8]]
    
    # Reordenar cronológicamente
    selected_messages.sort(key=lambda x: messages.index(x))
    
    return {"messages": selected_messages}
 
def extract_keywords(text: str) -> List[str]:
    """Extrae palabras clave simples del texto"""
    # Implementación simple - en producción usarías NLP más sofisticado
    words = text.lower().split()
    # Filtrar palabras comunes y mantener palabras significativas
    stop_words = {"el", "la", "de", "que", "y", "en", "un", "es", "se", "no", "te", "lo"}
    keywords = [word for word in words if len(word) > 3 and word not in stop_words]
    return list(set(keywords))[:10]  # Top 10 keywords únicos
 
def calculate_relevance(text: str, keywords: List[str]) -> float:
    """Calcula relevancia simple basada en keywords"""
    text_lower = text.lower()
    matches = sum(1 for keyword in keywords if keyword in text_lower)
    return matches / max(len(keywords), 1)
```


Mejores Prácticas
Empieza Simple: Comienza con ventana deslizante y evoluciona según necesidades

Mide el Impacto: Monitorea tokens usados, costos y calidad de respuestas

Ajusta Dinámicamente: Permite configuración de parámetros basada en feedback

Mantén Transparencia: Informa al usuario cuando se pierde contexto

Testa Exhaustivamente: Diferentes estrategias funcionan mejor para diferentes casos

Conclusión
La gestión efectiva de memoria en LangGraph va mucho más allá de simplemente mantener los últimos N mensajes. Las estrategias avanzadas como memoria de resumen, filtrado inteligente y ventanas adaptativas pueden transformar dramáticamente la experiencia del usuario, permitiendo conversaciones más largas, contextualizadas y costo-eficientes.

La clave está en entender tu caso de uso específico y elegir la combinación correcta de estrategias. Experimenta con diferentes enfoques, mide su impacto en la calidad de las respuestas y optimiza según los patrones de uso reales de tus usuarios.

En los próximos videos del curso, veremos cómo implementar algunas de estas estrategias paso a paso, incluyendo la integración con LLMs para operaciones como resumir y analizar relevancia de contexto.

### Memoria vectorial 
La memoria vectorial es una técnica avanzada que permite a los agentes recordar y recuperar información basada en similitud semántica en lugar de solo orden cronológico. Utilizando embeddings, los mensajes y documentos se convierten en vectores en un espacio multidimensional, lo que facilita la búsqueda de información relevante incluso si no coincide exactamente con las palabras utilizadas. Esta técnica es especialmente útil para manejar grandes volúmenes de datos y mantener conversaciones coherentes a lo largo del tiempo, ya que permite al agente acceder rápidamente a información relevante sin depender únicamente del historial lineal de mensajes.

