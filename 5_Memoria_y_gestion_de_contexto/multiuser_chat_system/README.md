# üß† Sistema de Chat Multi-Usuario con Memoria Vectorial Avanzada

> **Documentaci√≥n T√©cnica Detallada**
> Este documento proporciona una explicaci√≥n profunda de la arquitectura, componentes y flujo de datos del sistema. Dise√±ado para desarrolladores e ingenieros de IA que deseen comprender c√≥mo construir sistemas de memoria persistente.

---

## üìñ 1. Introducci√≥n y Filosof√≠a del Proyecto

Los LLMs (Large Language Models) tradicionales son "amn√©sicos" por dise√±o: cada nueva sesi√≥n es una pizarra en blanco. Si bien las ventanas de contexto han crecido (128k, 1M tokens), pasar todo el historial de conversaciones pasadas es ineficiente, costoso y lento.

Este proyecto resuelve el problema de la **continuidad** mediante una **Arquitectura de Memoria H√≠brida**:
1.  **Memoria Epis√≥dica (Corto Plazo)**: Gestionada por `LangGraph`, mantiene el contexto inmediato de la conversaci√≥n actual.
2.  **Memoria Sem√°ntica (Largo Plazo)**: Gestionada por `ChromaDB`, almacena hechos, preferencias y datos del usuario de forma permanente y consultable sem√°nticamente.

El objetivo es crear un asistente que no solo "chatee", sino que **conozca** al usuario a lo largo del tiempo.

---

## üèóÔ∏è 2. Arquitectura del Sistema

El sistema sigue una arquitectura modular desacoplada, donde la interfaz (Frontend) est√° separada de la l√≥gica de negocio (Backend/Core), unidas por un gestor de estado.

### Diagrama de Alto Nivel

```mermaid
graph TD
    subgraph "Frontend (Streamlit)"
        UI[üñ•Ô∏è Interfaz de Usuario]
        Session[üì¶ Session State]
    end

    subgraph "Orquestador (LangGraph)"
        Graph[üîÑ Grafo de Estados]
        Nodes[üìç Nodos de Procesamiento]
    end

    subgraph "Capa de Memoria (Memory Manager)"
        Ext[üîç Extractor de Hechos]
        Ret[üé£ Recuperador Sem√°ntico]
        VDB[(üß† ChromaDB - Vectores)]
        MetaDB[(üìÑ JSON/SQLite - Metadatos)]
    end

    subgraph "Modelos (OpenAI)"
        ChatModel[ü§ñ GPT-4o (Chat)]
        ExtractModel[‚õèÔ∏è GPT-3.5/4 (Extracci√≥n)]
        EmbedModel[üî¢ Text-Embedding-3 (Vectores)]
    end

    UI <--> Session
    Session <--> Graph
    Graph <--> Nodes
    Nodes <--> ChatModel
    Nodes <--> Ext
    Nodes <--> Ret
    Ext <--> ExtractModel
    Ret <--> EmbedModel
    Ret <--> VDB
    Ext <--> VDB
```

---

## üî¨ 3. An√°lisis Profundo de Componentes

A continuaci√≥n, desglosamos cada archivo y su responsabilidad t√©cnica.

### 3.1. `memory_manager.py`: El Hipocampo del Sistema

Este m√≥dulo es el cerebro de la memoria a largo plazo. No solo guarda texto, sino que **entiende** qu√© es importante guardar.

#### üß† Base de Datos Vectorial (ChromaDB)
Utilizamos **ChromaDB** como almac√©n vectorial.
*   **¬øQu√© es un Vector?**: Es una representaci√≥n num√©rica (lista de floats) del *significado* de un texto. Frases como "Me gustan los perros" y "Amo a los caninos" tendr√°n vectores muy cercanos matem√°ticamente, aunque no compartan palabras.
*   **Embeddings**: Usamos `OpenAIEmbeddings` (modelo `text-embedding-3-large`) para convertir texto en estos vectores.

#### ‚õèÔ∏è Sistema de Extracci√≥n Inteligente (`_init_extraction_system`)
En lugar de guardar *todo* lo que dice el usuario (lo cual llenar√≠a la base de datos de ruido), usamos un LLM secundario para filtrar.
*   **Prompt de Extracci√≥n**: Analiza el mensaje y decide si contiene informaci√≥n de categor√≠as espec√≠ficas: `personal`, `profesional`, `preferencias`, `hechos_importantes`.
*   **Salida Estructurada**: Usamos `PydanticOutputParser` para obligar al LLM a responder en un formato JSON estricto (`ExtractedMemory`), garantizando que siempre tengamos una categor√≠a y un nivel de importancia (1-5).

#### üìÇ Persistencia H√≠brida
*   **Vectores**: Se guardan en `users/{user_id}/chromadb`.
*   **Metadatos de Chat**: T√≠tulos de chats, fechas de creaci√≥n, etc., se guardan en `users/{user_id}/chats_meta.json` para un acceso r√°pido sin necesidad de inferencia vectorial.

---

### 3.2. `chatbot.py`: El Orquestador (LangGraph)

Aqu√≠ reside la l√≥gica conversacional. Usamos **LangGraph** en lugar de cadenas lineales (LangChain Chains) porque necesitamos un flujo c√≠clico y con estado.

#### üîÑ El Grafo de Estados (`StateGraph`)
El grafo define una m√°quina de estados por donde pasa cada mensaje.
*   **Estado (`MemoryState`)**: Es un diccionario tipado que viaja por los nodos. Contiene:
    *   `messages`: Lista de mensajes (User/AI).
    *   `vector_memories`: Memorias recuperadas de ChromaDB.
    *   `last_memory_extraction`: Para evitar procesar el mismo mensaje dos veces.

#### üìç Nodos del Grafo (Paso a Paso)

1.  **`memory_retrieval_node`**:
    *   Toma el √∫ltimo mensaje del usuario.
    *   Lo convierte en vector.
    *   Busca en ChromaDB los "recuerdos" m√°s similares sem√°nticamente.
    *   Inyecta estos recuerdos en el estado.

2.  **`context_optimization_node`**:
    *   Los LLMs tienen un l√≠mite de contexto. Si la conversaci√≥n es muy larga, este nodo usa `trim_messages` para recortar los mensajes m√°s antiguos, manteniendo siempre el mensaje del sistema y los m√°s recientes.

3.  **`response_generation_node`**:
    *   Construye el prompt final.
    *   **Inyecci√≥n de Contexto**: Toma las `vector_memories` recuperadas en el paso 1 y las inserta en el System Prompt. As√≠ el LLM "sabe" lo que record√≥.
    *   Genera la respuesta.

4.  **`memory_extraction_node`**:
    *   Este nodo corre *despu√©s* de generar la respuesta (o en paralelo conceptualmente).
    *   Llama al `memory_manager` para ver si el mensaje original del usuario ten√≠a algo digno de guardarse a largo plazo.
    *   Esto asegura que el aprendizaje sea continuo.

---

### 3.3. `app.py`: La Interfaz (Streamlit)

Streamlit funciona recargando todo el script en cada interacci√≥n. Esto presenta un desaf√≠o para mantener el estado.

#### üì¶ Gesti√≥n de Estado (`st.session_state`)
Para que el chatbot no se "reinicie" cada vez que pulsas un bot√≥n, usamos `st.session_state` intensivamente:
*   `current_user`: Qui√©n est√° logueado.
*   `chatbot`: La instancia de la clase `ModernChatbot`.
*   `memory_manager`: La instancia de `ModernMemoryManager`.
*   `chat_history`: Cache local de mensajes para renderizado r√°pido.

#### üé® UI Din√°mica
*   **Sidebar**: Cambia din√°micamente seg√∫n si hay un usuario seleccionado. Muestra el historial de chats cargado desde el JSON de metadatos.
*   **Chat Interface**: Renderiza los mensajes con estilo diferenciado (User vs Assistant). Muestra metadatos como "Memorias usadas" o "Contexto optimizado" debajo de cada respuesta para transparencia.

---

## üåä 4. Flujo de Datos: "Vida de un Mensaje"

Imagina que el usuario dice: *"Recu√©rdame comprar leche ma√±ana"*

1.  **UI**: `app.py` captura el texto y llama a `chatbot.chat()`.
2.  **LangGraph - Inicio**: Se inicializa el estado con el mensaje.
3.  **Recuperaci√≥n**:
    *   Se busca "comprar leche" en ChromaDB.
    *   Quiz√°s encuentra una nota antigua: "Prefiero leche de almendras".
    *   Este recuerdo se a√±ade al estado.
4.  **Optimizaci√≥n**: Se verifica que el historial total no exceda los tokens.
5.  **Generaci√≥n**:
    *   Prompt al LLM:
        *   *System*: "Eres un asistente... Sabes esto del usuario: 'Prefiero leche de almendras'."
        *   *User*: "Recu√©rdame comprar leche ma√±ana".
    *   El LLM responde: "Claro, te recordar√© comprar leche de almendras ma√±ana."
6.  **Extracci√≥n (Aprendizaje)**:
    *   El sistema analiza "Recu√©rdame comprar leche ma√±ana".
    *   Clasifica como `hechos_importantes`.
    *   Guarda el vector en ChromaDB.
7.  **UI**: Muestra la respuesta y un indicador "üß† 1 memoria usada".

---

## ‚öôÔ∏è 5. Configuraci√≥n (`config.py`)

El archivo `config.py` centraliza las variables cr√≠ticas para facilitar el mantenimiento.

| Variable | Descripci√≥n | Valor por Defecto |
| :--- | :--- | :--- |
| `DEFAULT_MODEL` | Modelo principal de chat | `gpt-5-nano` (o gpt-4o) |
| `MAX_VECTOR_RESULTS` | Cu√°ntos recuerdos recuperar | `3` |
| `MEMORY_CATEGORIES` | Categor√≠as de clasificaci√≥n | personal, profesional, etc. |
| `USERS_DIR` | Ruta de almacenamiento | `./users` |

---

## üöÄ 6. Gu√≠a de Instalaci√≥n y Uso

### Requisitos Previos
*   **Python 3.9+**: Necesario para las √∫ltimas versiones de LangChain.
*   **OpenAI API Key**: Cr√©ditos activos.

### Instalaci√≥n Paso a Paso

1.  **Clonar y Preparar Entorno**:
    ```bash
    git clone <repo>
    cd multiuser_chat_system
    python -m venv venv
    source venv/bin/activate  # Linux/Mac
    # venv\Scripts\activate  # Windows
    ```

2.  **Instalar Dependencias**:
    ```bash
    pip install streamlit langchain langgraph langchain-openai langchain-chroma chromadb python-dotenv pydantic
    ```

3.  **Configurar Secretos**:
    Crea un archivo `.env`:
    ```env
    OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxx
    ```

4.  **Ejecutar**:
    ```bash
    streamlit run app.py
    ```

---

## üîÆ 7. Extensibilidad y Futuro

Este sistema est√° dise√±ado para crecer:
*   **Cambiar Vector Store**: Cambiar ChromaDB por Pinecone o Weaviate es trivial modificando solo `_init_vector_db` en `memory_manager.py`.
*   **Modelos Locales**: Se puede reemplazar `ChatOpenAI` por `ChatOllama` para usar Llama 3 localmente, garantizando privacidad total.
*   **Herramientas (Tools)**: LangGraph permite a√±adir nodos de herramientas (b√∫squeda web, calendario) f√°cilmente al grafo.

---
---

## üõ†Ô∏è 8. Soluci√≥n de Problemas (Troubleshooting)

### üî¥ Error: `sqlite3.OperationalError: database is locked`
*   **Causa**: LangGraph usa SQLite para checkpoints. Si intentas abrir la misma base de datos desde m√∫ltiples hilos o procesos (ej. corriendo `streamlit run` dos veces), se bloquear√°.
*   **Soluci√≥n**:
    1.  Det√©n todos los procesos de terminal (`Ctrl+C`).
    2.  Verifica que no haya procesos zombies de python.
    3.  Reinicia la app: `streamlit run app.py`.

### üî¥ Error: `RateLimitError` (OpenAI)
*   **Causa**: Has excedido tu cuota de API o los l√≠mites por minuto (RPM).
*   **Soluci√≥n**:
    *   Verifica tu saldo en OpenAI Platform.
    *   En `config.py`, cambia `DEFAULT_MODEL` a uno m√°s barato/r√°pido como `gpt-3.5-turbo`.
    *   Implementa un "backoff exponencial" en `utils.py` (actualmente no implementado por defecto).

### üî¥ La memoria no parece persistir
*   **Causa**: ChromaDB requiere que se llame a `persist()` o se configure correctamente el directorio.
*   **Verificaci√≥n**:
    1.  Revisa la carpeta `users/{user_id}/chromadb`. Deber√≠a haber archivos `.bin` y `.sqlite`.
    2.  Si borras esta carpeta, el usuario perder√° su memoria a largo plazo.

---

## üìö 9. Referencia de API (Clases Principales)

### `ModernMemoryManager` (`memory_manager.py`)

| M√©todo | Firma | Descripci√≥n |
| :--- | :--- | :--- |
| `__init__` | `(user_id: str)` | Inicializa ChromaDB y el sistema de extracci√≥n para un usuario espec√≠fico. |
| `save_vector_memory` | `(text, metadata) -> str` | Guarda un fragmento de texto como vector. Retorna el ID de la memoria. |
| `search_vector_memory` | `(query, k=3) -> list` | Busca los `k` recuerdos m√°s similares sem√°nticamente a `query`. |
| `extract_and_store_memories` | `(user_message) -> bool` | **Core Logic**. Usa un LLM para analizar si el mensaje merece ser guardado. |
| `create_new_chat` | `(first_message) -> str` | Crea una nueva sesi√≥n y genera un t√≠tulo autom√°tico usando LLM. |

### `ModernChatbot` (`chatbot.py`)

| M√©todo | Firma | Descripci√≥n |
| :--- | :--- | :--- |
| `chat` | `(message, chat_id) -> dict` | Punto de entrada principal. Ejecuta el grafo de LangGraph. Retorna respuesta y metadatos. |
| `get_conversation_history` | `(chat_id, limit) -> list` | Recupera el historial formateado desde el estado de LangGraph. |
| `_create_app` | `() -> CompiledGraph` | Construye y compila el grafo de estados (Nodos + Aristas). |

---

## üîç 10. Inspecci√≥n de Base de Datos

Para depurar o auditar qu√© est√° guardando el sistema, puedes usar este script de utilidad (crear como `inspect_db.py`):

```python
import chromadb
from config import USERS_DIR
import os

def inspect_user_memory(user_id):
    path = os.path.join(USERS_DIR, user_id, "chromadb")
    client = chromadb.PersistentClient(path=path)
    collection = client.get_collection(f"memoria_{user_id}")
    
    print(f"--- Memorias de {user_id} ---")
    data = collection.get()
    for i, doc in enumerate(data['documents']):
        meta = data['metadatas'][i]
        print(f"[{meta['category'].upper()}] (Imp: {meta['importance']})")
        print(f"Contenido: {doc}")
        print("-" * 20)

# Uso
inspect_user_memory("usuario_ejemplo")
```

---
*Documentaci√≥n generada autom√°ticamente por Antigravity Agent.*
